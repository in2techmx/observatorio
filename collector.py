import os, json, datetime, time, urllib.request, hashlib, re, sys, math, struct, logging
import xml.etree.ElementTree as ET
from collections import defaultdict
from google import genai
from typing import List, Dict, Optional, Tuple

# ============================================================================
# CONFIGURACI√ìN DEL SISTEMA
# ============================================================================
MAX_PER_REGION_IN_AREA = 10
AREAS = ["Seguridad y Conflictos", "Econom√≠a y Sanciones", "Energ√≠a y Recursos", 
         "Soberan√≠a y Alianzas", "Tecnolog√≠a y Espacio", "Sociedad y Derechos"]
CACHE_DIR = "vector_cache"
HIST_DIR = "historico_noticias/diario"
LOG_FILE = "system_audit.log"
MAX_ITEMS_TOTAL = 2000  # L√≠mite de seguridad para memoria

# Configuraci√≥n de logging
for d in [CACHE_DIR, HIST_DIR]:
    os.makedirs(d, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

# ============================================================================
# FUENTES RSS - RED DE VIGILANCIA
# ============================================================================
FUENTES = {
    "USA": [
        "https://rss.nytimes.com/services/xml/rss/nyt/US.xml",
        "http://rss.cnn.com/rss/edition_us.rss",
        "https://feeds.washingtonpost.com/rss/politics",
        "https://www.reutersagency.com/feed/?best-topics=political-news"
    ],
    "RUSSIA": [
        "https://tass.com/rss/v2.xml",
        "http://en.kremlin.ru/events/president/news/feed",
        "https://themoscowtimes.com/rss/news",
        "https://sputniknews.com/export/rss2/archive/index.xml"
    ],
    "CHINA": [
        "https://www.scmp.com/rss/91/feed",
        "https://www.chinadaily.com.cn/rss/world_rss.xml",
        "https://www.globaltimes.cn/rss/china.xml"
    ],
    "EUROPE": [
        "https://www.theguardian.com/world/rss",
        "https://www.france24.com/en/rss",
        "https://rss.dw.com/xml/rss-en-all",
        "https://elpais.com/rss/elpais/inenglish.xml"
    ],
    "MID_EAST": [
        "https://www.aljazeera.com/xml/rss/all.xml",
        "https://www.trtworld.com/rss",
        "https://www.arabnews.com/cat/1/rss.xml"
    ],
    "GLOBAL": [
        "https://www.wired.com/feed/category/science/latest/rss",
        "https://techcrunch.com/feed/",
        "https://www.economist.com/sections/international/rss.xml"
    ]
}

# ============================================================================
# CLASE NewsItem - TRAZABILIDAD COMPLETA
# ============================================================================
class NewsItem:
    """Contenedor unificado para trazabilidad completa de cada se√±al"""
    
    def __init__(self, item_id: str, title: str, link: str, region: str, source_url: str):
        self.id = item_id
        self.original_title = self._sanitize_text(title)
        self.link = link if self._is_valid_url(link) else None
        self.region = region
        self.source_url = source_url
        self.translated_title: Optional[str] = None
        self.area: Optional[str] = None
        self.confidence: float = 0.0
        self.keywords: List[str] = []
        self.vector: Optional[List[float]] = None
        self.proximity: float = 50.0
        self.timestamp = datetime.datetime.now()
        self.processing_errors: List[str] = []
        
    @staticmethod
    def _is_valid_url(url: str) -> bool:
        """Validaci√≥n robusta de URL"""
        if not url or not isinstance(url, str):
            return False
        
        try:
            # Patr√≥n regex m√°s completo
            pattern = re.compile(
                r'^https?://'  # http:// o https://
                r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # dominio
                r'localhost|'  # localhost
                r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # IP
                r'(?::\d+)?'  # puerto opcional
                r'(?:/?|[/?]\S+)?$', 
                re.IGNORECASE
            )
            return bool(pattern.match(url))
        except:
            return False
    
    @staticmethod
    def _sanitize_text(text: str, max_length: int = 400) -> str:
        """Sanitizaci√≥n segura de texto"""
        if not text:
            return ""
        
        # Eliminar caracteres peligrosos
        text = re.sub(r'<[^>]+>', '', text)  # HTML tags
        text = re.sub(r'[\x00-\x1F\x7F]', '', text)  # Control characters
        text = re.sub(r'javascript:', '', text, flags=re.IGNORECASE)
        text = re.sub(r'<\s*script', '', text, flags=re.IGNORECASE)
        
        # Normalizar espacios y trim
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text[:max_length]
    
    def to_dict(self) -> Dict:
        """Serializaci√≥n para exportaci√≥n"""
        return {
            "id": self.id,
            "titulo": self.translated_title or self.original_title,
            "link": self.link,
            "bloque": self.region,
            "proximidad": round(self.proximity, 1),
            "confianza": round(self.confidence, 1),
            "keywords": self.keywords[:5],
            "area": self.area,
            "timestamp": self.timestamp.isoformat()
        }

# ============================================================================
# MOTOR PRINCIPAL - IroncladCollector CORREGIDO
# ============================================================================
class IroncladCollector:
    """Motor de inteligencia geopol√≠tica con correcciones cr√≠ticas aplicadas"""
    
    def __init__(self, api_key: str):
        self.client = genai.Client(api_key=api_key)
        self.active_items: List[NewsItem] = []
        self.stats = {
            "items_collected": 0,
            "items_classified": 0,
            "api_calls": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "errors": 0
        }
        self.start_time = time.time()
    
    # ========================================================================
    # UTILIDADES DE SEGURIDAD Y VALIDACI√ìN
    # ========================================================================
    def safe_json_extract(self, text: str) -> Optional[Dict]:
        """Extracci√≥n robusta de JSON con stack-based parsing"""
        if not text:
            return None
        
        # Intentar parseo directo primero
        text = text.strip()
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass
        
        # B√∫squeda de objetos JSON anidados
        stack = []
        start_idx = -1
        depth = 0
        
        for i, char in enumerate(text):
            if char == '{':
                if not stack:
                    start_idx = i
                stack.append(char)
                depth += 1
            elif char == '}':
                if stack:
                    stack.pop()
                    depth -= 1
                    if depth == 0 and start_idx != -1:
                        try:
                            candidate = text[start_idx:i+1]
                            return json.loads(candidate)
                        except json.JSONDecodeError:
                            # Continuar buscando
                            start_idx = -1
        
        logging.warning("No se pudo extraer JSON v√°lido")
        return None
    
    def validate_and_classify_area(self, area_name: str) -> Tuple[Optional[str], float]:
        """Validaci√≥n y clasificaci√≥n de √°rea con confianza"""
        if not area_name:
            return None, 0.0
        
        area_lower = area_name.lower().strip()
        
        # B√∫squeda exacta
        for area in AREAS:
            if area.lower() == area_lower:
                return area, 100.0
        
        # B√∫squeda parcial
        for area in AREAS:
            if area.lower() in area_lower or area_lower in area.lower():
                return area, 80.0
        
        # B√∫squeda por palabras clave
        keywords_map = {
            "Seguridad y Conflictos": ["militar", "defensa", "guerra", "conflicto", "terrorismo", "ej√©rcito", "ataque"],
            "Econom√≠a y Sanciones": ["econom√≠a", "finanzas", "sanciones", "mercado", "comercio", "inflaci√≥n", "bancos"],
            "Energ√≠a y Recursos": ["energ√≠a", "petr√≥leo", "gas", "miner√≠a", "clima", "renovable", "agua"],
            "Soberan√≠a y Alianzas": ["soberan√≠a", "alianza", "diplomacia", "tratado", "geopol√≠tica", "otan", "brics"],
            "Tecnolog√≠a y Espacio": ["tecnolog√≠a", "espacio", "digital", "sat√©lite", "ciber", "ia", "chips"],
            "Sociedad y Derechos": ["derechos", "humano", "social", "salud", "educaci√≥n", "protesta", "justicia"]
        }
        
        for area, keywords in keywords_map.items():
            if any(keyword in area_lower for keyword in keywords):
                # Calcular confianza basada en matches
                matches = sum(1 for keyword in keywords if keyword in area_lower)
                confidence = min(100.0, matches * 20.0)  # 20% por keyword
                return area, confidence
        
        return None, 0.0
    
    # ========================================================================
    # SISTEMA DE CACH√â DE EMBEDDINGS
    # ========================================================================
    def get_vector(self, text: str) -> Optional[List[float]]:
        """Obtiene vector de cach√© o None si no existe"""
        if not text:
            return None
        
        cache_key = hashlib.md5(text.encode('utf-8')).hexdigest()
        cache_path = os.path.join(CACHE_DIR, f"{cache_key}.bin")
        
        if not os.path.exists(cache_path):
            return None
        
        try:
            with open(cache_path, 'rb') as f:
                data = f.read()
                vector = list(struct.unpack(f'{len(data)//4}f', data))
            
            self.stats["cache_hits"] += 1
            return vector
        except Exception as e:
            logging.warning(f"Error cargando vector de cach√©: {e}")
            return None
    
    def save_vector(self, text: str, vector: List[float]) -> bool:
        """Guarda vector en cach√©"""
        if not text or not vector:
            return False
        
        try:
            cache_key = hashlib.md5(text.encode('utf-8')).hexdigest()
            cache_path = os.path.join(CACHE_DIR, f"{cache_key}.bin")
            
            with open(cache_path, 'wb') as f:
                f.write(struct.pack(f'{len(vector)}f', *vector))
            
            self.stats["cache_misses"] += 1
            return True
        except Exception as e:
            logging.error(f"Error guardando vector en cach√©: {e}")
            return False
    
    def cleanup_old_cache(self, days: int = 7):
        """Limpia cach√© antigua"""
        try:
            cutoff = time.time() - (days * 86400)
            deleted = 0
            
            for filename in os.listdir(CACHE_DIR):
                if filename.endswith('.bin'):
                    filepath = os.path.join(CACHE_DIR, filename)
                    if os.path.getmtime(filepath) < cutoff:
                        os.remove(filepath)
                        deleted += 1
            
            if deleted > 0:
                logging.info(f"Limpieza de cach√©: {deleted} archivos antiguos eliminados")
        except Exception as e:
            logging.error(f"Error en limpieza de cach√©: {e}")
    
    # ========================================================================
    # FASE 1: RECOLECCI√ìN DE SE√ëALES
    # ========================================================================
    def fetch_all_feeds(self) -> None:
        """Recolecci√≥n robusta de todos los feeds RSS"""
        logging.info("üì° FASE 1: Recolecci√≥n de se√±ales multi-fuente...")
        
        item_counter = 0
        
        for region, urls in FUENTES.items():
            for url in urls:
                if item_counter >= MAX_ITEMS_TOTAL:
                    logging.warning(f"L√≠mite m√°ximo de items alcanzado: {MAX_ITEMS_TOTAL}")
                    break
                
                try:
                    # Headers para evitar bloqueos
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                        'Accept': 'application/xml, text/xml, */*',
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Accept-Encoding': 'gzip, deflate',
                        'Connection': 'keep-alive'
                    }
                    
                    req = urllib.request.Request(url, headers=headers)
                    
                    with urllib.request.urlopen(req, timeout=15) as response:
                        # Detectar encoding
                        content_type = response.headers.get('Content-Type', '')
                        encoding = 'utf-8'
                        
                        if 'charset=' in content_type:
                            encoding = content_type.split('charset=')[-1].split(';')[0].strip()
                        
                        # Leer y decodificar
                        raw_data = response.read()
                        try:
                            xml_data = raw_data.decode(encoding)
                        except (UnicodeDecodeError, LookupError):
                            xml_data = raw_data.decode('utf-8', errors='ignore')
                        
                        # Parsear XML
                        root = ET.fromstring(xml_data)
                        
                        # Buscar items (compatible con RSS y Atom)
                        items = (root.findall('.//item') or 
                                root.findall('.//{*}item') or 
                                root.findall('.//entry') or 
                                root.findall('.//{*}entry'))
                        
                        for item in items[:12]:  # L√≠mite por feed
                            if item_counter >= MAX_ITEMS_TOTAL:
                                break
                            
                            # Extraer t√≠tulo
                            title_elem = (item.find('title') or 
                                         item.find('{*}title') or 
                                         item.find('.//{http://www.w3.org/2005/Atom}title'))
                            title = title_elem.text if title_elem is not None else ""
                            
                            # Extraer enlace
                            link_elem = (item.find('link') or 
                                        item.find('{*}link') or 
                                        item.find('.//{http://www.w3.org/2005/Atom}link'))
                            
                            if link_elem is not None:
                                if link_elem.text:
                                    link = link_elem.text.strip()
                                else:
                                    link = link_elem.get('href', '').strip()
                            else:
                                link = ""
                            
                            # Crear item solo si tiene datos v√°lidos
                            if title and link and len(title) > 10:
                                item_id = f"{region}_{item_counter}_{hashlib.md5(link.encode()).hexdigest()[:8]}"
                                news_item = NewsItem(item_id, title, link, region, url)
                                
                                if news_item.link:  # Solo si URL es v√°lida
                                    self.active_items.append(news_item)
                                    item_counter += 1
                    
                    logging.debug(f"Feed procesado: {url} ({len(items)} items)")
                    
                except urllib.error.URLError as e:
                    logging.warning(f"Error de red en feed {url}: {e}")
                    self.stats["errors"] += 1
                except ET.ParseError as e:
                    logging.warning(f"XML inv√°lido en feed {url}: {e}")
                    self.stats["errors"] += 1
                except Exception as e:
                    logging.error(f"Error inesperado en feed {url}: {e}")
                    self.stats["errors"] += 1
        
        self.stats["items_collected"] = len(self.active_items)
        logging.info(f"‚úÖ Recolecci√≥n completada: {self.stats['items_collected']} se√±ales v√°lidas")
    
    # ========================================================================
    # FASE 2: CLASIFICACI√ìN CON IA
    # ========================================================================
    def classify_with_ai(self) -> None:
        """Clasificaci√≥n por lotes con validaci√≥n robusta"""
        if not self.active_items:
            logging.warning("No hay items para clasificar")
            return
        
        logging.info(f"üîé FASE 2: Clasificaci√≥n IA ({len(self.active_items)} se√±ales)...")
        
        # Prompt optimizado con few-shot examples
        system_prompt = f"""Eres un analista de inteligencia geopol√≠tica. Clasifica cada titular:

√ÅREAS ESTRAT√âGICAS (SOLO UNA POR TITULAR):
{chr(10).join(f'- {area}' for area in AREAS)}

EJEMPLOS:
- "Ciberataque a infraestructura cr√≠tica" ‚Üí "Seguridad y Conflictos"
- "Sanciones econ√≥micas a banco central" ‚Üí "Econom√≠a y Sanciones"  
- "Acuerdo para exportaci√≥n de gas natural" ‚Üí "Energ√≠a y Recursos"
- "Firma de pacto de defensa mutua" ‚Üí "Soberan√≠a y Alianzas"
- "Lanzamiento de sat√©lite de comunicaciones" ‚Üí "Tecnolog√≠a y Espacio"
- "Protestas por derechos laborales" ‚Üí "Sociedad y Derechos"

INSTRUCCIONES:
1. TRADUCE al espa√±ol manteniendo significado exacto
2. ASIGNA EXACTAMENTE UNA √°rea estrat√©gica de la lista
3. CALIFICA tu confianza (0-100)
4. EXTRAE 3-5 palabras clave relevantes

RESPONDE SOLO CON JSON:
{{"res": [{{"id": "...", "area": "...", "titulo_es": "...", "confianza": 95, "keywords": ["kw1", "kw2"]}}]}}"""
        
        batch_size = 25  # Tama√±o seguro para Gemini Flash
        classified_count = 0
        
        for i in range(0, len(self.active_items), batch_size):
            batch = self.active_items[i:i+batch_size]
            
            # Construir prompt del batch
            batch_items = []
            for item in batch:
                batch_items.append(f"ID:{item.id}|{item.original_title}")
            
            batch_prompt = f"{system_prompt}\n\nTITULARES:\n" + "\n".join(batch_items)
            
            try:
                self.stats["api_calls"] += 1
                
                response = self.client.models.generate_content(
                    model="gemini-2.0-flash",
                    contents=batch_prompt,
                    config={
                        "temperature": 0.1,
                        "max_output_tokens": 2000,
                        "top_p": 0.95
                    }
                )
                
                # Parsear respuesta
                data = self.safe_json_extract(response.text)
                
                if data and 'res' in data:
                    for result in data['res']:
                        item_id = str(result.get('id', '')).strip()
                        
                        # Buscar el item correspondiente
                        target_item = next((it for it in batch if it.id == item_id), None)
                        
                        if target_item:
                            # Validar y clasificar √°rea
                            area_name = result.get('area', '')
                            classified_area, confidence = self.validate_and_classify_area(area_name)
                            
                            if classified_area and confidence >= 50:  # Umbral de confianza
                                target_item.area = classified_area
                                target_item.translated_title = result.get('titulo_es', '')
                                target_item.confidence = min(confidence, float(result.get('confianza', confidence)))
                                target_item.keywords = result.get('keywords', [])[:5]
                                
                                classified_count += 1
                                self.stats["items_classified"] += 1
                            else:
                                target_item.processing_errors.append(f"√Årea inv√°lida o confianza baja: {area_name}")
                
                logging.debug(f"Batch {i//batch_size + 1}: {len(data['res'] if data else [])} procesados")
                
            except Exception as e:
                logging.error(f"Error en batch {i//batch_size + 1}: {str(e)[:100]}")
                self.stats["errors"] += 1
                continue
            
            # Rate limiting respetuoso
            if i + batch_size < len(self.active_items):
                time.sleep(1.5)
        
        logging.info(f"‚úÖ Clasificaci√≥n completada: {classified_count} se√±ales clasificadas")
    
    # ========================================================================
    # FASE 3: AN√ÅLISIS DE FRICCI√ìN NARRATIVA (CORREGIDO)
    # ========================================================================
    def calculate_proximity_optimized(self) -> None:
        """C√°lculo de proximidad optimizado O(n) con correcciones"""
        logging.info("üìê FASE 3: An√°lisis de fricci√≥n narrativa...")
        
        # Limpiar cach√© antigua
        self.cleanup_old_cache()
        
        # Agrupar items por √°rea
        items_by_area = defaultdict(list)
        for item in self.active_items:
            if item.area and item.area in AREAS:
                items_by_area[item.area].append(item)
        
        # Procesar cada √°rea por separado
        for area, items in items_by_area.items():
            if len(items) < 2:  # Necesitamos al menos 2 items para comparar
                for item in items:
                    item.proximity = 50.0
                continue
            
            logging.debug(f"Procesando √°rea '{area}' con {len(items)} items")
            
            # ================================================================
            # PASO 1: OBTENER EMBEDDINGS (CON CACH√â)
            # ================================================================
            items_with_text = [item for item in items if item.translated_title]
            
            # Obtener embeddings con cach√©
            for item in items_with_text:
                cached_vector = self.get_vector(item.translated_title)
                if cached_vector:
                    item.vector = cached_vector
                else:
                    # Marcar para embedding batch
                    item.vector = None
            
            # Agrupar textos que necesitan embedding
            texts_to_embed = [item.translated_title for item in items_with_text if item.vector is None]
            
            if texts_to_embed:
                try:
                    self.stats["api_calls"] += 1
                    
                    emb_response = self.client.models.embed_content(
                        model="text-embedding-004",
                        contents=texts_to_embed,
                        config={'task_type': 'RETRIEVAL_DOCUMENT'}
                    )
                    
                    # Asignar vectores y guardar en cach√©
                    text_idx = 0
                    for item in items_with_text:
                        if item.vector is None:  # Necesita embedding
                            if text_idx < len(emb_response.embeddings):
                                item.vector = emb_response.embeddings[text_idx].values
                                self.save_vector(item.translated_title, item.vector)
                                text_idx += 1
                
                except Exception as e:
                    logging.error(f"Error en embeddings para √°rea {area}: {e}")
                    self.stats["errors"] += 1
                    # Fallback: vectores aleatorios normalizados
                    for item in items_with_text:
                        if item.vector is None:
                            import random
                            random.seed(hash(item.translated_title))
                            item.vector = [random.uniform(-0.1, 0.1) for _ in range(768)]
            
            # ================================================================
            # PASO 2: CALCULAR CENTROIDES POR REGI√ìN (OPTIMIZADO)
            # ================================================================
            # Filtrar items con vector v√°lido
            valid_items = [item for item in items_with_text if item.vector and len(item.vector) > 0]
            
            if len(valid_items) < 2:
                for item in items:
                    item.proximity = 50.0
                continue
            
            # Agrupar vectores por regi√≥n
            vectors_by_region = defaultdict(list)
            for item in valid_items:
                vectors_by_region[item.region].append(item.vector)
            
            # Pre-calcular centroides por regi√≥n
            region_centroids = {}
            for region, vectors in vectors_by_region.items():
                if vectors:
                    # Calcular promedio por dimensi√≥n
                    centroid = []
                    for dim_idx in range(len(vectors[0])):  # Asume todos los vectores tienen misma dimensi√≥n
                        dim_values = [vec[dim_idx] for vec in vectors]
                        centroid.append(sum(dim_values) / len(dim_values))
                    region_centroids[region] = centroid
            
            # ================================================================
            # PASO 3: CALCULAR PROXIMIDAD (CORRECCI√ìN APLICADA)
            # ================================================================
            for item in valid_items:
                current_region = item.region
                current_vector = item.vector
                
                # Recolectar centroides de otras regiones
                other_centroids = []
                for region, centroid in region_centroids.items():
                    if region != current_region:
                        other_centroids.append(centroid)
                
                if not other_centroids:
                    # Solo esta regi√≥n habla del tema
                    item.proximity = 50.0
                    continue
                
                # Calcular centroide global de "otras regiones"
                global_centroid = []
                for dim_idx in range(len(current_vector)):
                    dim_values = [centroid[dim_idx] for centroid in other_centroids]
                    global_centroid.append(sum(dim_values) / len(dim_values))
                
                # CALCULO CORREGIDO: Similitud coseno [-1, 1] -> Proximidad [0, 100]
                # 1. Calcular producto punto
                dot_product = sum(a * b for a, b in zip(current_vector, global_centroid))
                
                # 2. Calcular magnitudes
                magnitude_current = math.sqrt(sum(x * x for x in current_vector))
                magnitude_global = math.sqrt(sum(x * x for x in global_centroid))
                
                # 3. Calcular similitud coseno
                if magnitude_current * magnitude_global > 0:
                    cosine_similarity = dot_product / (magnitude_current * magnitude_global)
                    
                    # 4. CORRECCI√ìN CR√çTICA: Mapear [-1, 1] a [0, 100]
                    # F√≥rmula: proximity = (similarity + 1) * 50
                    # similarity = -1 -> proximity = 0
                    # similarity = 0 -> proximity = 50  
                    # similarity = 1 -> proximity = 100
                    raw_proximity = (cosine_similarity + 1) * 50
                    
                    # 5. Asegurar rango [0, 100] (por errores de punto flotante)
                    item.proximity = max(0.0, min(100.0, raw_proximity))
                else:
                    item.proximity = 50.0
            
            # Para items sin vector, asignar valor neutro
            for item in items:
                if not hasattr(item, 'proximity') or item.proximity is None:
                    item.proximity = 50.0
        
        logging.info("‚úÖ An√°lisis de proximidad completado")
    
    # ========================================================================
    # FASE 4: GENERACI√ìN DE MICRO-INFORMES PARA NETFLIX
    # ========================================================================
    def generate_micro_reports(self) -> Dict[str, str]:
        """Genera micro-informes por √°rea para la interfaz Netflix"""
        logging.info("üé¨ Generando micro-informes para interfaz Netflix...")
        
        # Agrupar items por √°rea
        items_by_area = defaultdict(list)
        for item in self.active_items:
            if item.area and item.area in AREAS and item.proximity is not None:
                items_by_area[item.area].append(item)
        
        micro_reports = {}
        
        for area, items in items_by_area.items():
            if not items:
                micro_reports[area] = f"Actividad limitada detectada en {area}."
                continue
            
            # Calcular m√©tricas del √°rea
            avg_proximity = sum(item.proximity for item in items) / len(items)
            region_counts = defaultdict(int)
            keyword_counts = defaultdict(int)
            
            for item in items:
                region_counts[item.region] += 1
                for keyword in item.keywords[:3]:
                    keyword_counts[keyword.lower()] += 1
            
            # Top regiones y keywords
            top_regions = sorted(region_counts.items(), key=lambda x: x[1], reverse=True)[:3]
            top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:3]
            
            # Determinar nivel de consenso
            if avg_proximity > 80:
                consensus = "ALTO CONSENSO"
                emoji = "üü¢"
            elif avg_proximity > 60:
                consensus = "CONSENSO MODERADO"
                emoji = "üü°"
            elif avg_proximity > 40:
                consensus = "TENSI√ìN DETECTADA"
                emoji = "üü†"
            else:
                consensus = "FRICCI√ìN SEVERA"
                emoji = "üî¥"
            
            # Determinar tendencia
            trend = "‚Üë" if avg_proximity > 60 else "‚Üì" if avg_proximity < 40 else "‚Üí"
            
            # Construir micro-informe
            regions_str = ", ".join([r[0] for r in top_regions])
            keywords_str = ", ".join([k[0] for k in top_keywords])
            
            micro_report = (
                f"{emoji} {consensus} | "
                f"Bloques activos: {regions_str} | "
                f"Temas: {keywords_str} | "
                f"Tendencia: {trend}"
            )
            
            micro_reports[area] = micro_report[:150]  # Limitar longitud
        
        return micro_reports
    
    # ========================================================================
    # FASE 5: EXPORTACI√ìN DE RESULTADOS
    # ========================================================================
    def export_results(self) -> Dict:
        """Exporta resultados en formato para frontend Netflix"""
        logging.info("üíæ Exportando resultados...")
        
        # Generar micro-informes
        micro_reports = self.generate_micro_reports()
        
        # Agrupar items por √°rea para el carrusel
        carousel_data = []
        
        for area in AREAS:
            # Filtrar items del √°rea
            area_items = [item for item in self.active_items if item.area == area]
            
            if not area_items:
                continue
            
            # Convertir a formato de part√≠culas
            particles = []
            for item in area_items:
                particle = item.to_dict()
                
                # A√±adir etiqueta de sesgo basada en proximidad
                if particle["proximidad"] > 80:
                    particle["sesgo"] = "Consenso Global"
                elif particle["proximidad"] > 65:
                    particle["sesgo"] = "Alineaci√≥n"
                elif particle["proximidad"] > 50:
                    particle["sesgo"] = "Tensi√≥n Moderada"
                elif particle["proximidad"] > 35:
                    particle["sesgo"] = "Divergencia"
                else:
                    particle["sesgo"] = "Contraste Radical"
                
                particles.append(particle)
            
            # Ordenar por proximidad (m√°s consensuados primero)
            particles.sort(key=lambda x: x["proximidad"], reverse=True)
            
            # Color por √°rea
            area_colors = {
                "Seguridad y Conflictos": "#ef4444",
                "Econom√≠a y Sanciones": "#3b82f6",
                "Energ√≠a y Recursos": "#10b981",
                "Soberan√≠a y Alianzas": "#f59e0b",
                "Tecnolog√≠a y Espacio": "#8b5cf6",
                "Sociedad y Derechos": "#ec4899"
            }
            
            carousel_data.append({
                "area": area,
                "punto_cero": micro_reports.get(area, f"An√°lisis de {area} actualizado."),
                "color": area_colors.get(area, "#666666"),
                "total_particulas": len(particles),
                "particulas": particles[:30]  # Top 30 por √°rea
            })
        
        # Metadatos de ejecuci√≥n
        execution_time = time.time() - self.start_time
        
        meta = {
            "updated": datetime.datetime.now().isoformat(),
            "execution_seconds": round(execution_time, 2),
            "stats": self.stats,
            "version": "IroncladCollector v2.0 (Corregido)",
            "total_areas": len(carousel_data),
            "total_particles": sum(area["total_particulas"] for area in carousel_data),
            "cache_efficiency": round(
                self.stats["cache_hits"] / (self.stats["cache_hits"] + self.stats["cache_misses"]) * 100, 
                2
            ) if (self.stats["cache_hits"] + self.stats["cache_misses"]) > 0 else 0
        }
        
        result = {
            "carousel": carousel_data,
            "meta": meta
        }
        
        # Guardar archivo principal
        try:
            with open("gravity_carousel.json", "w", encoding="utf-8") as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            logging.info("‚úÖ gravity_carousel.json actualizado")
        except Exception as e:
            logging.error(f"Error guardando archivo principal: {e}")
        
        # Guardar hist√≥rico diario
        try:
            date_str = datetime.datetime.now().strftime("%Y-%m-%d")
            hist_path = os.path.join(HIST_DIR, f"{date_str}.json")
            
            with open(hist_path, "w", encoding="utf-8") as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            logging.info(f"‚úÖ Hist√≥rico guardado: {hist_path}")
        except Exception as e:
            logging.error(f"Error guardando hist√≥rico: {e}")
        
        # Guardar resumen ejecutivo
        try:
            summary = {
                "timestamp": meta["updated"],
                "areas_summary": [
                    {
                        "area": area["area"],
                        "particle_count": area["total_particulas"],
                        "avg_proximity": round(
                            sum(p["proximidad"] for p in area["particulas"]) / area["total_particulas"], 
                            2
                        ) if area["total_particulas"] > 0 else 0,
                        "micro_report": area["punto_cero"][:100]
                    }
                    for area in carousel_data
                ],
                "performance": {
                    "execution_time": meta["execution_seconds"],
                    "items_processed": self.stats["items_collected"],
                    "classification_rate": round(
                        self.stats["items_classified"] / self.stats["items_collected"] * 100, 
                        2
                    ) if self.stats["items_collected"] > 0 else 0,
                    "cache_efficiency": meta["cache_efficiency"]
                }
            }
            
            with open("executive_summary.json", "w", encoding="utf-8") as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            logging.info("‚úÖ Resumen ejecutivo generado")
        except Exception as e:
            logging.error(f"Error guardando resumen: {e}")
        
        return result
    
    # ========================================================================
    # EJECUCI√ìN PRINCIPAL
    # ========================================================================
    def run(self) -> Optional[Dict]:
        """Ejecuta el pipeline completo"""
        print("\n" + "="*70)
        print("üöÄ IRONCLAD COLLECTOR - Sistema de Inteligencia Geopol√≠tica")
        print("="*70)
        
        try:
            # Pipeline principal
            self.fetch_all_feeds()
            self.classify_with_ai()
            self.calculate_proximity_optimized()
            result = self.export_results()
            
            # Reporte final
            total_time = time.time() - self.start_time
            
            print("\n" + "="*70)
            print("‚úÖ AN√ÅLISIS COMPLETADO EXITOSAMENTE")
            print("="*70)
            
            print(f"üìä ESTAD√çSTICAS:")
            print(f"   ‚Ä¢ Tiempo total: {total_time:.1f} segundos")
            print(f"   ‚Ä¢ Se√±ales recolectadas: {self.stats['items_collected']}")
            print(f"   ‚Ä¢ Se√±ales clasificadas: {self.stats['items_classified']}")
            print(f"   ‚Ä¢ Llamadas API: {self.stats['api_calls']}")
            print(f"   ‚Ä¢ Hits de cach√©: {self.stats['cache_hits']}")
            print(f"   ‚Ä¢ Eficiencia de cach√©: {self.stats.get('cache_efficiency', 0):.1f}%")
            
            if result:
                total_particles = sum(len(area["particulas"]) for area in result["carousel"])
                print(f"   ‚Ä¢ Part√≠culas totales: {total_particles}")
                print(f"   ‚Ä¢ √Åreas activas: {len(result['carousel'])}")
            
            print(f"\nüìÅ ARCHIVOS GENERADOS:")
            print("   1. gravity_carousel.json (datos principales)")
            print("   2. executive_summary.json (resumen ejecutivo)")
            print(f"   3. {HIST_DIR}/YYYY-MM-DD.json (hist√≥rico diario)")
            print(f"   4. {LOG_FILE} (registros de sistema)")
            
            print("\n‚ö†Ô∏è  NOTA: Revisa gravity_carousel.json para visualizar el radar")
            print("="*70 + "\n")
            
            return result
            
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è  Ejecuci√≥n interrumpida por el usuario")
            logging.info("Ejecuci√≥n interrumpida por el usuario")
            return None
        except Exception as e:
            print(f"\n‚ùå ERROR CR√çTICO: {str(e)[:200]}")
            logging.error(f"Error en ejecuci√≥n principal: {e}", exc_info=True)
            return None

# ============================================================================
# PUNTO DE ENTRADA
# ============================================================================
def main():
    """Funci√≥n principal con manejo de errores"""
    api_key = os.environ.get("GEMINI_API_KEY")
    
    if not api_key:
        print("‚ùå ERROR: Variable de entorno GEMINI_API_KEY no encontrada")
        print("\nPor favor, configura tu API key:")
        print("  Linux/Mac:  export GEMINI_API_KEY='tu-clave-aqui'")
        print("  Windows:    set GEMINI_API_KEY=tu-clave-aqui")
        print("\nO ejecuta: GEMINI_API_KEY='tu-clave' python ironclad_collector.py")
        sys.exit(1)
    
    # Inicializar y ejecutar collector
    collector = IroncladCollector(api_key)
    result = collector.run()
    
    if result:
        print("üéØ Inteligencia geopol√≠tica actualizada exitosamente!")
        sys.exit(0)
    else:
        print("‚ö†Ô∏è  El an√°lisis encontr√≥ problemas. Revisa los logs.")
        sys.exit(1)

if __name__ == "__main__":
    main()
